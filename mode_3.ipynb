{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c13cec-0463-459e-8236-286a8c437ec3",
   "metadata": {},
   "source": [
    "Build advanced Model#2 (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24fc71ee-ceb1-42c8-96b8-b2aaafd9d3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 28.58\n",
      "R2 Score: 0.3592\n",
      "\n",
      "Random Forest Feature Importances:\n",
      "                        Feature  Importance\n",
      "2                 Mile_Time_Min    0.339326\n",
      "3         Weekly_Exercise_Hours    0.188428\n",
      "11               Access_To_Bike    0.140509\n",
      "1                           GPA    0.106326\n",
      "10          Has_Sibling_In_Club    0.054237\n",
      "6           Previous_Sport_None    0.048887\n",
      "5   Previous_Sport_CrossCountry    0.047354\n",
      "0                   Grade_Level    0.030849\n",
      "9          Previous_Sport_Track    0.018287\n",
      "4     Previous_Sport_Basketball    0.011856\n",
      "7         Previous_Sport_Soccer    0.008930\n",
      "8       Previous_Sport_Swimming    0.005011\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# --- 1. Re-generate Identical Data ---\n",
    "np.random.seed(42)\n",
    "num_students = 300\n",
    "grades = np.random.choice([9, 10, 11, 12], size=num_students, p=[0.3, 0.3, 0.2, 0.2])\n",
    "gpa = np.clip(np.random.normal(3.3, 0.4, num_students), 2.0, 4.0)\n",
    "mile_time_min = np.clip(np.random.normal(9.0, 1.5, num_students), 5.5, 15.0)\n",
    "weekly_exercise_hours = np.clip(np.random.normal(5, 2, num_students), 0, 15)\n",
    "has_sibling_in_club = np.random.choice([0, 1], size=num_students, p=[0.85, 0.15])\n",
    "access_to_bike = np.random.choice([0, 1], size=num_students, p=[0.6, 0.4])\n",
    "sports = ['None', 'Soccer', 'CrossCountry', 'Swimming', 'Basketball', 'Track']\n",
    "previous_sport = np.random.choice(sports, size=num_students, p=[0.3, 0.2, 0.15, 0.1, 0.1, 0.15])\n",
    "\n",
    "# Target Formula\n",
    "base_score = 50\n",
    "noise = np.random.normal(0, 5, num_students)\n",
    "sport_boost = {'None': 0, 'Basketball': 2, 'Soccer': 4, 'Swimming': 5, 'Track': 8, 'CrossCountry': 10}\n",
    "sport_scores = np.array([sport_boost[s] for s in previous_sport])\n",
    "\n",
    "score = base_score + (gpa * 5) + ((12.0 - mile_time_min) * 3) + (weekly_exercise_hours * 2) + \\\n",
    "        (has_sibling_in_club * 10) + (access_to_bike * 5) + sport_scores + noise\n",
    "final_score = np.clip(score, 0, 100)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Grade_Level': grades, 'GPA': gpa, 'Mile_Time_Min': mile_time_min,\n",
    "    'Weekly_Exercise_Hours': weekly_exercise_hours, 'Previous_Sport': previous_sport,\n",
    "    'Has_Sibling_In_Club': has_sibling_in_club, 'Access_To_Bike': access_to_bike,\n",
    "    'Recruit_Potential_Score': final_score\n",
    "})\n",
    "\n",
    "# --- 2. Split Data ---\n",
    "X = df.drop('Recruit_Potential_Score', axis=1)\n",
    "y = df['Recruit_Potential_Score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 3. Setup Random Forest Pipeline ---\n",
    "numeric_features = ['Grade_Level', 'GPA', 'Mile_Time_Min', 'Weekly_Exercise_Hours']\n",
    "categorical_features = ['Previous_Sport']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Define Pipeline with Random Forest\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# --- 4. Train Model ---\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- 5. Evaluate ---\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "# --- 6. Feature Importance ---\n",
    "# Extracting feature names again for display\n",
    "ohe = rf_pipeline.named_steps['preprocessor'].named_transformers_['cat']\n",
    "cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "feature_names = numeric_features + cat_names + ['Has_Sibling_In_Club', 'Access_To_Bike']\n",
    "\n",
    "# Get importances from the Random Forest step\n",
    "importances = rf_pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "imp_df = imp_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importances:\")\n",
    "print(imp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071215e-f4fc-4ec3-afa0-45ff016d2f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
